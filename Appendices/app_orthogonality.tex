\section{An expression for the discrimination ratio}
\label{app:orth}
In this section, we prove the following proposition.

\begin{prop}[An expression for the ratio of kernel elements]{Let $\mL$ be a Laplacian matrix 
\begin{equation}
\mathcal{L}_{ij} =
 \begin{cases}
       {k_{ij}} & i\neq j, \ k_{ij}\ge0\\
       {-\sum_j k_{ij}} & i=j\\
\end{cases}
\end{equation}}
representing a fully connected continuous time Markov chain.  Note that the nullspace of $\mL$ is one dimensional; let it have basis $\rho.$  The ratio of any two elements of $\rho$ is given by
\[
\frac{\rho_i}{\rho_j} = \frac{\norm{v_i - \proj_\sij(v_i)}}{\norm{v_j - \proj_\sij(v_j)}}.
\]
\begin{proof}
The result follows from three equalities:
\begin{equation}\label{three_eq}
\begin{aligned}
\frac{\rho_i}{\rho_j} &= \frac{\det(\mL^{kj})}{\det(\mL^{ki})}  \ \ \ \forall \ k \in 1\ldots N\\
& = \frac{\vol(P(\mL^{0j}))}{\vol(P(\mL^{0i}))} \\
& =  \frac{\norm{v_i - \proj_\sij(v_i)}}{\norm{v_j - \proj_\sij(v_j)}}
\end{aligned}
\end{equation}
where $\mL^{ki}$ represents the matrix formed by removing row $k$ and column $i$ from matrix $\mL,$ and $\mL^{0k}$ is formed from $\mL$ by removing column $k$ only.  For matrix $A$, $\volp{A}$ represents the volume the parallelotope formed by the columns of $A$;  vector $v_i$ represents the $i$th column of $\mL;$ and $\proj_\sij(v_i)$ denotes the projection of vector $v_i$ onto the $n-2$ dimensional subspace $S$ spanned by the columns of $\mL$ remove $i,j.$  Please note that in our notation, $A^{xy}\neq A^{x,y}.$  The former (no superscript commas) denotes the matrix $A$ remove row $x$ and column $y.$  The latter (commas in superscript) denotes the matrix $A$ remove {\it column} $x$ and column $y.$
\end{proof}
\end{prop}

We proceed by proving each of the three equalities in Equation \ref{three_eq}.  To prove the first equality, it will be useful to have the definition of the adjugate matrix at hand.

\begin{defn}[Adjugate matrix]{
The components of the adjugate of a matrix $A$, $\adj(A),$ are given by taking the transpose of the cofactor matrix, $C$, of $A$:
\begin{equation}\label{adj}
\begin{aligned}
\adj(A)_{ij} &= C_{ji}\\
&= \det(A^{ji})\\
\end{aligned}
\end{equation}
where $A^{ji}$ is denotes the $(n-1) \times (n-1)$ matrix resulting form removing row $j$ and column $i$ from $A.$

For the second equality in Equation \ref{adj}, we have recalled that the elements of the cofactor matrix $C_{ij}$ of $A$ are given by (up to sign):
\[
C_{ij} = \det(A^{ij})
\]
where $A^{ij}$ denotes the $(n-1) \times (n-1)$ matrix resulting form removing row $i$ and column $j$ from $A.$
}
\end{defn}

We can now prove the first equality in \ref{three_eq}.

\begin{prop}[Discrimination ratio in terms of determinants with column and row cuts]{ We aim to demonstrate that
\[
\frac{\rho_i}{\rho_j} = \frac{\det(\mL^{kj})}{\det(\mL^{ki})}  \ \ \ \forall \ k \in 1\ldots N.
\]
}
\begin{proof}
The proposition was proved in \cite{Mirzaev2013}.  We include the argument here for completeness.  

By the Matrix-Tree theorem, the rank of a fully-connected $N$ dimensional Laplacian matrix is $N-1.$  The nullspace is therefore one-dimensional, and can be represented by a single basis vector $\rho.$  

It will suffice to prove that $\rho_i = \det(\mL^{ki}).$ Recall the Laplace expansion for the determinant:
\begin{equation}\label{expand}
\begin{aligned}
\adj(\mL)\cdot \mL = \mL\cdot \adj(\mL) &= \det(\mL)\cdot I\\ 
&= 0_{n\times n},
\end{aligned}
\end{equation}
where $0_{n\times n}$ denotes the $n$ by $n$ zero matrix and the final equality follows from $\mL$ not being full rank, hence $\det(\mL) = 0.$

Consider that $\mL\cdot \adj(\mL)=0$ implies that $\mL v=0_{n\times 1}$ for all $v$ which are columns of $\adj(\mL).$  That is: the columns of $\adj(\mL)$ are equal to $\rho.$  This gives the result.
\end{proof}
\end{prop}

We now prove the second equality in Equation \ref{three_eq}.

\begin{prop}[Discrimination ratio in terms of column cuts only]{We now wish to demonstrate that the equality presented in the previous proposition does not require the removal of some row $k$ \cite{Drucker2015}:
\[
\begin{aligned}
\frac{\det(\mL^{ki})}{\det(\mL^{kj})} &= \frac{\volp{\mL^{ki}}}{\volp{\mL^{kj}}}\\
&= \frac{\volp{\mL^{0i}}}{\volp{\mL^{0j}}}\\
\end{aligned}
\]
}
\begin{proof}
The first equality is a common characterization of the determinant.  The second result follows from a series of equalities 
\[
\begin{aligned}
\frac{\vol(P(\mL^{0i}))}{\vol(P(\mL^{0j}))} &= \frac{\sqrt{\det[(L^{0i})^T(L^{0i})]}}{\sqrt{\det[(L^{0j})^T(L^{0j})]}}\\
& = \sqrt{\frac{\sum_k (\det[L^{ki})]^2} {\sum_k (\det[L^{kj})]^2} }\\
& = \sqrt{\frac{N(\det(L^{ki}))^2} {N(\det(L^{kj}))^2} } = \frac{\det(\mL^{ki})}{\det(\mL^{kj})}\\
\end{aligned}
\]
where: the first equality is by definition of a polytope volume generated by a non-square matrix; the second equality results from applying the Cauchy-Binet formula; the third equality follows from noting that $\det(\mL^{ki}) = \det(\mL^{k'i}), \ \forall \ k,k' \in 1\ldots N.$
\end{proof}
\end{prop}

We now prove the final equality in Equation \ref{three_eq}.  First, it is useful to recall the base-height formula for determinants.

\begin{fact}[The base-height formula]{The determinant of a matrix $A$ can be written as
\[
\det(A) = \prod_i \lVert a_i \rVert
\]
where ${a_i}$ is a vector representing the component of $v_i$ that is perpendicular to the subspace spanned by the $N-i$ vectors \{$v_{i+1},\cdots,v_n$\}.  Crucially, this procedure can be done by selecting the $v_i$ in any order \cite{Gover2010}.
\begin{proof}
Geometrically, the determinant of a matrix $A$ having columns $v_i$ can be thought of as the volume of the  parallelotope generated by the $v_i.$  Consider a parallelotope $P(A)$ generated by vectors $\{v_{1},\cdots,v_n\}.$  $P(A)$ can also be thought of as a prism with base generated by the vectors $\{v_{2},\cdots,v_n\}$ and height equal to the magnitude of the component of $v_1$ perpendicular to the span of $\{v_{2},\cdots,v_n\}$.  It follows that
\begin{eqnarray*}
\vol_n(P(A)) = &\vol_{n-1}(P(\{v_{2},\cdots,v_n\})) \cdot \\ &\lVert v_1 - \proj(v_1; v_{2},\cdots,v_n)  \rVert
\end{eqnarray*}
And of course we can carry out this procedure successively for $\vol_{n-1}, \vol_{n-2},\ldots$.  This gives the desired result.
\end{proof}
}
\end{fact}

\begin{prop}[Discriminatory ratio in terms of normalized projections]{Finally, we demonstrate that
\[
\frac{\volp{\mL^{0i}}}{\volp{\mL^{0j}}} = \frac{\norm{v_j - \proj_S(v_j)}}{\norm{v_i - \proj_S(v_i)}}
\]
}
\begin{proof}
The result follows directly from the base-height formula for determinants.
\[
\begin{aligned}
\frac{\det(\mL^{0i})}{\det(\mL^{0j})} &= \frac{\norm{v_j - \proj_\sij(v_j)}\cdot \vol_{n-2}P(\{v_l\}_{l\neq i,j})}{\norm{v_i - \proj_\sij(v_i)}\cdot \vol_{n-2}P(\{v_l\}_{l\neq i,j})}\\
 &= \frac{\norm{v_j - \proj_\sij(v_j)}}{\norm{v_i - \proj_\sij(v_i)}}
\end{aligned}
\]
where $\proj_\sij(v_j)$ denotes the projection of vector $v_j$ onto the subspace spanned by the vectors of matrix $\sij,$ formed by deleting columns $i,\ j$ from $\mL.$
Notice that in the numerator, we have chosen to begin the base-height iteration with vector $v_j.$  Because $L^{0i}$ already has column $i$ removed, this procedure yields - in the numerator - a polytope base generated by the non-$i,j$ columns in $\mL.$  In the denominator, beginning the base-height iteration $v_i$ also yields a polytope base generated by the non-$i,j$ columns.  These bases cancel to give the desired result.
\end{proof}
\end{prop}




